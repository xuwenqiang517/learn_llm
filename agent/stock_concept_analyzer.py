"""
å¤§ç›˜æ¦‚å¿µåˆ†æè®¡ç®—å·¥å…·

ã€æ ¸å¿ƒç›®æ ‡ã€‘
å®šä½ç›®å‰çš„å¤§ç›˜ä¸»çº¿è¶‹åŠ¿ï¼Œé€šè¿‡åˆ†æå„ä¸ªæ¦‚å¿µæ¿å—çš„è¡¨ç°ï¼Œå¸®åŠ©è¯†åˆ«å¸‚åœºçƒ­ç‚¹å’ŒæŠ•èµ„æœºä¼šã€‚

ã€æµç¨‹ã€‘
1.åŸºäºæœ¬åœ°ç¼“å­˜æ•°æ®ï¼ˆ.temp/data/ï¼‰ è¿‡æ»¤å‡ºæœ€è¿‘5ä¸ªäº¤æ˜“æ—¥çš„è‚¡ç¥¨æ•°æ®
2.æ’é™¤æ‰å¼‚å¸¸è‚¡ç¥¨ï¼ŒåŒ…æ‹¬
   - STè‚¡ç¥¨ï¼ˆåç§°åŒ…å«'ST'æˆ–'*ST'ï¼‰
   - æ–°è‚¡ï¼ˆä»£ç ä»¥9å¼€å¤´ï¼Œå¦‚920045ï¼‰
   - åœç‰Œè‚¡ç¥¨ï¼ˆä»»æ„ä¸€å¤©æ— æ•°æ®ï¼‰
   - å¼‚å¸¸æ¶¨è·Œå¹…ï¼ˆå•æ—¥æ¶¨è·Œå¹…è¶…è¿‡21%ï¼‰
   - é»‘åå•æ¦‚å¿µï¼ˆå¦‚"æ˜¨æ—¥è§¦æ¿"ç­‰æ— å‚è€ƒä»·å€¼çš„æ¦‚å¿µï¼‰
3.ç”¨æ­£å¸¸çš„è‚¡ç¥¨æ•°æ®è¿›è¡Œåˆ†æï¼Œè®¡ç®—æ¯ä¸ªæ¦‚å¿µçš„æŒ‡æ ‡ï¼ŒæŒ‰ç´¯è®¡å¹³å‡æ¶¨è·Œå¹…æ’åºï¼Œè¯†åˆ«çƒ­é—¨æ¦‚å¿µï¼Œå–top10
4.åˆ†æç»“æœæŒ‰jsonæ ¼å¼è¿”å›ï¼ŒåªåŒ…å«Top 10æ¦‚å¿µçš„åŸºæœ¬ä¿¡æ¯ï¼š
   - æ¦‚å¿µåç§°
   - æœ€è¿‘5ä¸ªäº¤æ˜“æ—¥ç´¯è®¡æ¶¨è·Œå¹…
   - å¯¹åº”ä¸Šæ¶¨è‚¡ç¥¨æ•°é‡
   - å¯¹åº”ä¸‹è·Œè‚¡ç¥¨æ•°é‡
   æ”¯æŒtool/mcp/main ç”¨äºåç»­agentè°ƒç”¨å’Œç›´æ¥è°ƒç”¨
5. å¯è§†åŒ–è¾“å‡º ä¿å­˜åˆ°å½“å‰é¡¹ç›®çš„.temp/output/tools/ç›®å½•
   - ç”ŸæˆPNGæ ¼å¼å›¾è¡¨ï¼ŒåŒ…å«3ä¸ªå­å›¾ï¼š
     * å­å›¾1ï¼šæ¦‚å¿µç´¯è®¡å¹³å‡æ¶¨è·Œå¹…ï¼ˆTop Nï¼Œæ¨ªå‘æŸ±çŠ¶å›¾ï¼Œçº¢ç»¿é…è‰²ï¼‰
     * å­å›¾2ï¼šæ¦‚å¿µæ¶¨è·Œç»Ÿè®¡ï¼ˆä¸Šæ¶¨/ä¸‹è·Œæ¬¡æ•°å¯¹æ¯”ï¼‰
     * å­å›¾3ï¼šè‚¡ç¥¨è¯¦æƒ…è¡¨æ ¼ æŒ‰5å¤©ç´¯è®¡æ¶¨å¹…æ’åºï¼ˆæ¯ä¸ªæ¦‚å¿µæ˜¾ç¤ºæœ€å¤š20åªè‚¡ç¥¨ï¼‰
   - è¡¨æ ¼ç‰¹æ€§ï¼š
     * ä½¿ç”¨å®é™…æ—¥æœŸï¼ˆMM-DDæ ¼å¼ï¼‰ä½œä¸ºè¡¨å¤´
     * åˆå¹¶è‚¡ç¥¨åç§°å’Œä»£ç ï¼ˆå¦‚ï¼šè´µå·èŒ…å°(600519)ï¼‰
     * æ¦‚å¿µåç§°ä¸æ—¥æœŸåœ¨åŒä¸€è¡Œï¼Œè“è‰²èƒŒæ™¯ä½œä¸ºè¡¨å¤´
     * æ·»åŠ äº”æ—¥ç´¯è®¡æ¶¨è·Œå¹…åˆ—ï¼ˆæ”¾åœ¨æœ€å‰é¢ï¼‰
     * äº¤æ›¿è¡Œé¢œè‰²æé«˜å¯è¯»æ€§

"""

import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List, Any, Tuple
from collections import defaultdict

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from tqdm import tqdm

sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.json_util import JsonUtil
from utils.file_util import FileUtil
from utils.log_util import LogUtil

logger = LogUtil.get_logger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ==================== ç›®å½•ç»“æ„å®šä¹‰ ====================
BASE_DIR = Path(__file__).parent.parent
TEMP_DIR = BASE_DIR / ".temp"
DATA_DIR = TEMP_DIR / "data"
BASE_DATA_DIR = DATA_DIR / "base"
DAILY_DATA_DIR = DATA_DIR / "daily"
OUTPUT_DIR = TEMP_DIR / "output"
TOOLS_OUTPUT_DIR = OUTPUT_DIR / "tools"

# ç¡®ä¿ç›®å½•å­˜åœ¨
FileUtil.ensure_dirs(TOOLS_OUTPUT_DIR)

# åŸºç¡€æ•°æ®æ–‡ä»¶è·¯å¾„
STOCK_LIST_FILE = BASE_DATA_DIR / "stock_list.json"
INDUSTRY_MAP_FILE = BASE_DATA_DIR / "industry_map.json"
CONCEPT_MAP_FILE = BASE_DATA_DIR / "concept_map.json"

# æ¦‚å¿µé»‘åå•
BLACKLIST_CONCEPTS = {
    'æ˜¨æ—¥è¿æ¿', 'æ˜¨æ—¥æ¶¨åœ', 'æ˜¨æ—¥æ¶¨åœ_å«ä¸€å­—', 'æ˜¨æ—¥è¿æ¿_å«ä¸€å­—',
    'ä»Šæ—¥æ¶¨åœ', 'ä»Šæ—¥è¿æ¿', 'è¿‘æœŸå¼ºåŠ¿è‚¡', 'è¿‘æœŸæ´»è·ƒè‚¡',
    'èèµ„èåˆ¸', 'æ²ªè‚¡é€š', 'æ·±è‚¡é€š', 'æ¸¯è‚¡é€š', 'é¾™è™æ¦œ', 'æœºæ„é‡ä»“',
    'æ˜¨æ—¥è§¦æ¿'
}


# ==================== è¾…åŠ©å‡½æ•° ====================

def _get_trading_days(days: int = 10) -> List[str]:
    """
    è·å–äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆä»å®é™…æ•°æ®ç›®å½•ä¸­è·å–ï¼‰
    
    Args:
        days: å‘å‰æŸ¥æ‰¾çš„å¤©æ•°
        
    Returns:
        äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰ï¼Œä»æ–°åˆ°æ—§æ’åº
    """
    trading_days = []
    
    # ä»æ•°æ®ç›®å½•ä¸­è·å–å®é™…å­˜åœ¨çš„æ—¥æœŸ
    if DAILY_DATA_DIR.exists():
        # è·å–æ‰€æœ‰æ—¥æœŸç›®å½•
        date_dirs = sorted([d.name for d in DAILY_DATA_DIR.iterdir() if d.is_dir()], reverse=True)
        trading_days = date_dirs[:days]
    
    return trading_days


def _get_stock_daily(symbol: str, date: str) -> Optional[Dict]:
    """
    ä»ç¼“å­˜è·å–å•åªè‚¡ç¥¨å•æ—¥æ•°æ®
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        date: æ—¥æœŸï¼ˆYYYYMMDDï¼‰
        
    Returns:
        è‚¡ç¥¨æ•°æ®å­—å…¸ï¼Œä¸å­˜åœ¨è¿”å›None
    """
    cache_file = DAILY_DATA_DIR / date / f"{symbol}.json"
    return JsonUtil.load(cache_file)


def _load_stock_data_from_cache(symbol: str, dates: List[str]) -> Optional[pd.DataFrame]:
    """
    ä»æœ¬åœ°ç¼“å­˜åŠ è½½è‚¡ç¥¨å†å²æ•°æ®
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        dates: æ—¥æœŸåˆ—è¡¨
        
    Returns:
        è‚¡ç¥¨æ•°æ®DataFrameï¼Œå¤±è´¥è¿”å›None
    """
    records = []
    for date in dates:
        data = _get_stock_daily(symbol, date)
        if data is not None:
            records.append(data)
    
    if not records:
        return None
    
    df = pd.DataFrame(records)
    df = df.sort_values('date').reset_index(drop=True)
    return df


def _filter_concepts(concepts: List[str]) -> List[str]:
    """
    è¿‡æ»¤æ‰é»‘åå•æ¦‚å¿µ
    
    Args:
        concepts: æ¦‚å¿µåˆ—è¡¨
        
    Returns:
        è¿‡æ»¤åçš„æ¦‚å¿µåˆ—è¡¨
    """
    if not concepts:
        return []
    return [c for c in concepts if c not in BLACKLIST_CONCEPTS]


def _is_st_stock(name: str) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦ä¸ºSTè‚¡ç¥¨
    
    Args:
        name: è‚¡ç¥¨åç§°
        
    Returns:
        Trueè¡¨ç¤ºæ˜¯STè‚¡ç¥¨
    """
    return 'ST' in name.upper() or '*ST' in name.upper()


def _is_abnormal_stock(code: str, name: str, daily_changes: List[Dict], dates: List[str]) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚å¸¸è‚¡ç¥¨ï¼ˆSTè‚¡ç¥¨ã€æ–°è‚¡ã€åœç‰Œã€å¼‚å¸¸æ¶¨è·Œå¹…ç­‰ï¼‰
    
    Args:
        code: è‚¡ç¥¨ä»£ç 
        name: è‚¡ç¥¨åç§°
        daily_changes: æ¯æ—¥æ¶¨è·Œå¹…æ•°æ®
        dates: éœ€è¦æ£€æŸ¥çš„æ—¥æœŸåˆ—è¡¨
        
    Returns:
        Trueè¡¨ç¤ºæ˜¯å¼‚å¸¸è‚¡ç¥¨
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºSTè‚¡ç¥¨
    if _is_st_stock(name):
        return True
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°è‚¡ï¼ˆä»£ç ä»¥9å¼€å¤´ï¼Œå¦‚920045ï¼‰
    if code.startswith('9'):
        return True
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»æ„ä¸€å¤©æ²¡æ•°æ®ï¼ˆåœç‰Œï¼‰
    for date in dates:
        has_data = False
        for dc in daily_changes:
            if dc.get('date', '') == date:
                has_data = True
                break
        if not has_data:
            return True
    
    # æ£€æŸ¥æ¶¨è·Œå¹…æ˜¯å¦è¶…è¿‡21%
    for dc in daily_changes:
        change_pct = dc.get('change_pct', 0)
        if isinstance(change_pct, (int, float)):
            if abs(change_pct) > 21:
                return True
        elif isinstance(change_pct, str):
            try:
                pct_value = float(change_pct.replace('%', '').replace('+', ''))
                if abs(pct_value) > 21:
                    return True
            except:
                pass
    
    return False


def _calculate_5day_cumulative_change(daily_changes: List[Dict]) -> float:
    """
    è®¡ç®—5æ—¥ç´¯è®¡æ¶¨è·Œå¹…
    
    Args:
        daily_changes: æ¯æ—¥æ¶¨è·Œå¹…æ•°æ®åˆ—è¡¨
        
    Returns:
        5æ—¥ç´¯è®¡æ¶¨è·Œå¹…
    """
    valid_changes = []
    for dc in daily_changes:
        change_pct = dc.get('change_pct', 0)
        if isinstance(change_pct, (int, float)):
            valid_changes.append(change_pct)
        elif isinstance(change_pct, str):
            try:
                pct_value = float(change_pct.replace('%', '').replace('+', ''))
                valid_changes.append(pct_value)
            except:
                pass
    
    if valid_changes:
        return round(sum(valid_changes), 2)
    return 0.0


def _analyze_concept(concept_name: str, stock_codes: List[str], 
                    stock_list: pd.DataFrame, dates: List[str],
                    concept_map: Dict[str, List[str]]) -> Optional[Dict]:
    """
    åˆ†æå•ä¸ªæ¦‚å¿µï¼ˆå…ˆè¿‡æ»¤å¼‚å¸¸è‚¡ç¥¨ï¼Œå†è®¡ç®—æŒ‡æ ‡ï¼‰
    
    Args:
        concept_name: æ¦‚å¿µåç§°
        stock_codes: è¯¥æ¦‚å¿µä¸‹çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
        stock_list: è‚¡ç¥¨åˆ—è¡¨DataFrame
        dates: éœ€è¦åˆ†æçš„æ—¥æœŸåˆ—è¡¨ï¼ˆæœ€è¿‘5ä¸ªäº¤æ˜“æ—¥ï¼‰
        concept_map: æ¦‚å¿µæ¿å—æ˜ å°„
        
    Returns:
        æ¦‚å¿µåˆ†æç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å›None
    """
    if not stock_codes:
        return None
    
    # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤å¼‚å¸¸è‚¡ç¥¨ï¼ˆSTã€æ–°è‚¡ã€åœç‰Œã€å¼‚å¸¸æ¶¨è·Œå¹…ï¼‰
    valid_stocks = []
    for code in stock_codes:
        stock_info = stock_list[stock_list['code'] == code]
        if stock_info.empty:
            continue
        
        name = stock_info.iloc[0]['name']
        
        # è·å–è¯¥è‚¡ç¥¨æœ€è¿‘5ä¸ªäº¤æ˜“æ—¥çš„æ¶¨è·Œå¹…æ•°æ®
        daily_changes = []
        for date in dates:
            data = _get_stock_daily(code, date)
            if data is not None:
                daily_changes.append({
                    'date': date,
                    'change_pct': data.get('change_pct', 0)
                })
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚å¸¸è‚¡ç¥¨
        if not _is_abnormal_stock(code, name, daily_changes, dates):
            valid_stocks.append({
                'code': code,
                'name': name,
                'daily_changes': daily_changes,
                'cumulative_5day': _calculate_5day_cumulative_change(daily_changes)
            })
    
    if not valid_stocks:
        return None
    
    # ç¬¬äºŒæ­¥ï¼šåŸºäºè¿‡æ»¤åçš„æœ‰æ•ˆè‚¡ç¥¨è®¡ç®—æŒ‡æ ‡
    valid_stock_count = len(valid_stocks)
    
    # ç»Ÿè®¡æ¯ä¸ªäº¤æ˜“æ—¥çš„æ¶¨è·Œæƒ…å†µ
    daily_stats = []
    
    for date in dates:
        up_count = 0
        down_count = 0
        flat_count = 0
        total_change = 0.0
        valid_data_count = 0
        
        for stock in valid_stocks:
            daily_changes = stock.get('daily_changes', [])
            for dc in daily_changes:
                if dc.get('date', '') == date:
                    change_pct = dc.get('change_pct', 0)
                    total_change += change_pct
                    valid_data_count += 1
                    
                    if change_pct > 0:
                        up_count += 1
                    elif change_pct < 0:
                        down_count += 1
                    else:
                        flat_count += 1
                    break
        
        if valid_data_count > 0:
            avg_change = total_change / valid_data_count
        else:
            avg_change = 0.0
        
        daily_stats.append({
            'date': date,
            'up_count': up_count,
            'down_count': down_count,
            'flat_count': flat_count,
            'avg_change': round(avg_change, 2),
            'valid_data_count': valid_data_count
        })
    
    # è®¡ç®—æœ€è¿‘5ä¸ªäº¤æ˜“æ—¥çš„ç´¯è®¡å¹³å‡æ¶¨è·Œå¹…
    if daily_stats:
        total_avg_change = sum(d['avg_change'] for d in daily_stats)
        total_up = sum(d['up_count'] for d in daily_stats)
        total_down = sum(d['down_count'] for d in daily_stats)
    else:
        total_avg_change = 0.0
        total_up = 0
        total_down = 0
    
    # å‡†å¤‡è‚¡ç¥¨è¯¦æƒ…ï¼ˆæŒ‰5æ—¥ç´¯è®¡æ¶¨è·Œå¹…æ’åºï¼‰
    stock_details_sorted = sorted(valid_stocks, key=lambda x: x.get('cumulative_5day', 0), reverse=True)
    stock_details = []
    for stock in stock_details_sorted:
        stock_details.append({
            'code': stock['code'],
            'name': stock['name'],
            'daily_changes': stock['daily_changes'],
            'cumulative_5day': stock['cumulative_5day']
        })
    
    return {
        'concept_name': concept_name,
        'stock_count': valid_stock_count,
        'total_avg_change': round(total_avg_change, 2),
        'total_up_count': total_up,
        'total_down_count': total_down,
        'daily_stats': daily_stats,
        'stock_details': stock_details
    }


def _format_concept_result_for_chart(concept_results: List[Dict], dates: List[str]) -> str:
    """
    æ ¼å¼åŒ–æ¦‚å¿µåˆ†æç»“æœä¸ºJSONï¼ˆç”¨äºå›¾è¡¨ç”Ÿæˆï¼ŒåŒ…å«å®Œæ•´æ•°æ®ï¼‰
    
    Args:
        concept_results: æ¦‚å¿µåˆ†æç»“æœåˆ—è¡¨
        dates: æ—¥æœŸåˆ—è¡¨
        
    Returns:
        JSONæ ¼å¼çš„å­—ç¬¦ä¸²
    """
    if not concept_results:
        return JsonUtil.dumps({"message": "æœªæ‰¾åˆ°æ¦‚å¿µæ•°æ®"})
    
    # æŒ‰ç´¯è®¡å¹³å‡æ¶¨è·Œå¹…æ’åºï¼Œå–Top 20ç”¨äºå›¾è¡¨
    concept_results_sorted = sorted(
        concept_results,
        key=lambda x: x['total_avg_change'],
        reverse=True
    )[:20]
    
    # ä¸ºå›¾è¡¨å‡†å¤‡å®Œæ•´æ•°æ®
    concepts = []
    for concept in concept_results_sorted:
        daily_data = []
        for stat in concept['daily_stats'][-5:]:
            date_str = datetime.strptime(stat['date'], "%Y%m%d").strftime("%Y-%m-%d")
            daily_data.append({
                "date": date_str,
                "avg_change": f"{stat['avg_change']:+.2f}%",
                "up_count": stat['up_count'],
                "down_count": stat['down_count'],
                "flat_count": stat['flat_count'],
                "valid_data_count": stat['valid_data_count']
            })
        
        # æ·»åŠ è‚¡ç¥¨è¯¦æƒ…
        stock_details = []
        for stock in concept.get('stock_details', []):
            stock_daily_changes = []
            for dc in stock.get('daily_changes', []):
                stock_daily_changes.append({
                    "date": dc['date'],
                    "change_pct": f"{dc['change_pct']:+.2f}%"
                })
            
            stock_details.append({
                "code": stock['code'],
                "name": stock['name'],
                "daily_changes": stock_daily_changes,
                "cumulative_5day": f"{stock.get('cumulative_5day', 0):+.2f}%"
            })
        
        concepts.append({
            "concept_name": concept['concept_name'],
            "stock_count": concept['stock_count'],
            "total_avg_change": f"{concept['total_avg_change']:+.2f}%",
            "total_up_count": concept['total_up_count'],
            "total_down_count": concept['total_down_count'],
            "daily_stats": daily_data,
            "stock_details": stock_details
        })
    
    result = {
        "query_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total_concepts": len(concept_results),
        "analysis_days": len(dates),
        "concepts": concepts
    }
    
    return JsonUtil.dumps(result, indent=2)


def _format_concept_result(concept_results: List[Dict], dates: List[str],
                          compress: bool = False) -> str:
    """
    æ ¼å¼åŒ–æ¦‚å¿µåˆ†æç»“æœä¸ºJSONï¼ˆåªè¿”å›Top 10æ¦‚å¿µçš„åŸºæœ¬ä¿¡æ¯ï¼‰
    
    Args:
        concept_results: æ¦‚å¿µåˆ†æç»“æœåˆ—è¡¨
        dates: æ—¥æœŸåˆ—è¡¨
        compress: æ˜¯å¦å‹ç¼©æ•°æ®ï¼ˆå‡å°‘tokenæ¶ˆè€—ï¼‰
        
    Returns:
        JSONæ ¼å¼çš„å­—ç¬¦ä¸²
    """
    if not concept_results:
        return JsonUtil.dumps({"message": "æœªæ‰¾åˆ°æ¦‚å¿µæ•°æ®"})
    
    # æŒ‰ç´¯è®¡å¹³å‡æ¶¨è·Œå¹…æ’åºï¼Œå–Top 10
    concept_results_sorted = sorted(
        concept_results,
        key=lambda x: x['total_avg_change'],
        reverse=True
    )[:10]
    
    # åªè¿”å›Top 10æ¦‚å¿µçš„åŸºæœ¬ä¿¡æ¯
    concepts = []
    for concept in concept_results_sorted:
        concepts.append({
            "concept_name": concept['concept_name'],
            "total_avg_change": f"{concept['total_avg_change']:+.2f}%",
            "total_up_count": concept['total_up_count'],
            "total_down_count": concept['total_down_count']
        })
    
    result = {
        "query_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total_concepts": len(concept_results),
        "analysis_days": len(dates),
        "concepts": concepts
    }
    
    return JsonUtil.dumps(result, indent=None if compress else 2)


# ==================== å›¾è¡¨ç”Ÿæˆ ====================

def generate_chart_from_results(result: Dict, save_path: Optional[Path] = None, top_n: int = 20) -> str:
    """
    ä»æœç´¢ç»“æœç”Ÿæˆå›¾è¡¨
    
    Args:
        result: analyze_concepts è¿”å›çš„ç»“æœå­—å…¸
        save_path: å¯é€‰çš„æ–‡ä»¶ä¿å­˜è·¯å¾„
        top_n: æ˜¾ç¤ºå‰Nä¸ªæ¦‚å¿µï¼Œé»˜è®¤20
        
    Returns:
        å›¾ç‰‡æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
    """
    concepts = result.get('concepts', [])
    if not concepts:
        logger.warning("æœªæ‰¾åˆ°æ¦‚å¿µæ•°æ®")
        return "æœªæ‰¾åˆ°æ¦‚å¿µæ•°æ®"
    
    total_concepts = result.get('total_concepts', len(concepts))
    query_time = result.get('query_time', datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    analysis_days = result.get('analysis_days', 5)
    
    # æŒ‰ç´¯è®¡å¹³å‡æ¶¨è·Œå¹…æ’åºï¼Œå–å‰Nä¸ª
    concepts_sorted = sorted(
        concepts,
        key=lambda x: float(x.get('total_avg_change', '0%').replace('%', '').replace('+', '')),
        reverse=True
    )[:top_n]
    
    # å‡†å¤‡æ•°æ®
    concept_names = [c.get('concept_name', '')[:10] for c in concepts_sorted]
    total_avg_changes = [float(c.get('total_avg_change', '0%').replace('%', '').replace('+', '')) for c in concepts_sorted]
    stock_counts = [c.get('stock_count', 0) for c in concepts_sorted]
    up_counts = [c.get('total_up_count', 0) for c in concepts_sorted]
    down_counts = [c.get('total_down_count', 0) for c in concepts_sorted]
    
    # åˆ›å»ºæ›´å¤§çš„å›¾è¡¨ä»¥å®¹çº³è‚¡ç¥¨è¯¦æƒ…
    fig = plt.figure(figsize=(20, 24))
    gs = fig.add_gridspec(3, 1, height_ratios=[1, 1, 3], hspace=0.3)
    
    fig.suptitle(f'å¤§ç›˜æ¦‚å¿µè¶‹åŠ¿åˆ†æ (Top {top_n})\næŸ¥è¯¢æ—¶é—´: {query_time} | åˆ†æå‘¨æœŸ: æœ€è¿‘{analysis_days}ä¸ªäº¤æ˜“æ—¥ | æ€»æ¦‚å¿µæ•°: {total_concepts}', 
                 fontsize=16, fontweight='bold', y=0.995)
    
    # å­å›¾1: ç´¯è®¡å¹³å‡æ¶¨è·Œå¹…
    ax1 = fig.add_subplot(gs[0])
    colors1 = ['green' if x >= 0 else 'red' for x in total_avg_changes]
    bars1 = ax1.barh(range(len(concept_names)), total_avg_changes, color=colors1, alpha=0.7)
    ax1.set_yticks(range(len(concept_names)))
    ax1.set_yticklabels(concept_names, fontsize=10)
    ax1.set_xlabel('ç´¯è®¡å¹³å‡æ¶¨è·Œå¹… (%)', fontsize=11)
    ax1.set_title(f'æ¦‚å¿µç´¯è®¡å¹³å‡æ¶¨è·Œå¹… (Top {top_n})', fontsize=12, fontweight='bold')
    ax1.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
    ax1.grid(axis='x', alpha=0.3)
    
    # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
    for i, (bar, val) in enumerate(zip(bars1, total_avg_changes)):
        ax1.text(val, i, f'{val:+.2f}%', 
                va='center', ha='left' if val >= 0 else 'right', 
                fontsize=9, fontweight='bold')
    
    # å­å›¾2: æ¶¨è·Œç»Ÿè®¡
    ax2 = fig.add_subplot(gs[1])
    x = range(len(concept_names))
    width = 0.35
    bars2_up = ax2.bar([i - width/2 for i in x], up_counts, width, label='ä¸Šæ¶¨æ¬¡æ•°', color='red', alpha=0.7)
    bars2_down = ax2.bar([i + width/2 for i in x], down_counts, width, label='ä¸‹è·Œæ¬¡æ•°', color='green', alpha=0.7)
    ax2.set_xticks(x)
    ax2.set_xticklabels(concept_names, rotation=45, ha='right', fontsize=10)
    ax2.set_ylabel('æ¬¡æ•°', fontsize=11)
    ax2.set_title(f'æ¦‚å¿µæ¶¨è·Œç»Ÿè®¡ (Top {top_n})', fontsize=12, fontweight='bold')
    ax2.legend(loc='upper right')
    ax2.grid(axis='y', alpha=0.3)
    
    # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar in bars2_up:
        height = bar.get_height()
        if height > 0:
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom', fontsize=8)
    
    for bar in bars2_down:
        height = bar.get_height()
        if height > 0:
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom', fontsize=8)
    
    # å­å›¾3: è‚¡ç¥¨è¯¦æƒ…è¡¨æ ¼
    ax3 = fig.add_subplot(gs[2])
    ax3.axis('off')
    ax3.set_title(f'è‚¡ç¥¨è¯¦æƒ… (Top {top_n})', fontsize=12, fontweight='bold', pad=20)
    
    # ä»daily_statsä¸­è·å–æ—¥æœŸåˆ—è¡¨ç”¨äºè¡¨å¤´
    date_headers = []
    date_map = {}  # ç”¨äºåŒ¹é…è‚¡ç¥¨æ•°æ®çš„æ—¥æœŸæ˜ å°„
    
    if concepts_sorted and concepts_sorted[0].get('daily_stats'):
        daily_stats = concepts_sorted[0]['daily_stats']
        for stat in daily_stats:
            date_str = stat.get('date', '')
            if date_str:
                try:
                    date_obj = datetime.strptime(date_str, "%Y-%m-%d")
                    mm_dd = date_obj.strftime('%m-%d')
                    date_headers.append(mm_dd)
                    # åˆ›å»ºæ—¥æœŸæ˜ å°„ï¼šMM-DD -> YYYYMMDD
                    date_map[mm_dd] = date_obj.strftime('%Y%m%d')
                except:
                    date_headers.append(date_str)
    
    # å¦‚æœæ²¡æœ‰è·å–åˆ°æ—¥æœŸï¼Œä½¿ç”¨é»˜è®¤å€¼
    if len(date_headers) < analysis_days:
        date_headers = [f'D{i+1}' for i in range(analysis_days)]
    
    # ä¸ºæ¯ä¸ªæ¦‚å¿µåˆ›å»ºè‚¡ç¥¨è¯¦æƒ…è¡¨æ ¼
    table_data = []
    row_colors = []
    
    for idx, concept in enumerate(concepts_sorted):
        concept_name = concept.get('concept_name', '')
        stock_details = concept.get('stock_details', [])
        
        # æ·»åŠ æ¦‚å¿µæ ‡é¢˜è¡Œï¼ˆåˆå¹¶æ¦‚å¿µåç§°å’Œè¡¨å¤´ï¼‰
        table_data.append([f'{concept_name}', 'äº”æ—¥ç´¯è®¡'] + date_headers)
        row_colors.append('#4A90E2')
        
        # æ·»åŠ è‚¡ç¥¨è¯¦æƒ…ï¼ˆæŒ‰5æ—¥ç´¯è®¡æ¶¨è·Œå¹…æ’åºï¼Œæ¯ä¸ªæ¦‚å¿µæœ€å¤šæ˜¾ç¤º20åªï¼‰
        for stock in stock_details[:20]:
            name = stock.get('name', '')
            code = stock.get('code', '')
            daily_changes = stock.get('daily_changes', [])
            cumulative_5day = stock.get('cumulative_5day', 0)
            
            # åˆå¹¶åç§°å’Œä»£ç 
            name_code = f'{name}({code})'
            
            # æ·»åŠ äº”æ—¥ç´¯è®¡æ¶¨è·Œå¹…
            if isinstance(cumulative_5day, str) and '%' in cumulative_5day:
                avg_value = cumulative_5day
            else:
                avg_value = f'{cumulative_5day:+.2f}%'
            
            # æŒ‰ç…§è¡¨å¤´æ—¥æœŸé¡ºåºè·å–æ¶¨è·Œå¹…æ•°æ®
            changes = []
            for header in date_headers:
                # ä»date_mapä¸­è·å–å¯¹åº”çš„YYYYMMDDæ ¼å¼æ—¥æœŸ
                yyyymmdd = date_map.get(header, '')
                if yyyymmdd:
                    # åœ¨daily_changesä¸­æŸ¥æ‰¾å¯¹åº”æ—¥æœŸçš„æ•°æ®
                    found = False
                    for dc in daily_changes:
                        if dc.get('date', '') == yyyymmdd:
                            change_pct = dc.get('change_pct', 0)
                            if isinstance(change_pct, str):
                                changes.append(change_pct)
                            else:
                                changes.append(f'{change_pct:+.2f}%')
                            found = True
                            break
                    if not found:
                        changes.append('--')
                else:
                    changes.append('--')
            
            # æ·»åŠ æ•°æ®è¡Œï¼šåç§°ä»£ç  + äº”æ—¥ç´¯è®¡ + å„æ—¥æ¶¨è·Œå¹…
            table_data.append([name_code, avg_value] + changes)
            row_colors.append('#FFFFFF' if len(table_data) % 2 == 0 else '#F0F0F0')
        
        # æ·»åŠ ç©ºè¡Œåˆ†éš”
        table_data.append([''] + [''] * (len(date_headers) + 1))
        row_colors.append('#FFFFFF')
    
    # åˆ›å»ºè¡¨æ ¼
    table = ax3.table(cellText=table_data, cellLoc='left', loc='upper left',
                      colWidths=[0.2] + [0.1] * len(date_headers) + [0.1])
    
    # è®¾ç½®è¡¨æ ¼æ ·å¼
    for (row, col), cell in table.get_celld().items():
        if row < len(table_data):
            cell.set_facecolor(row_colors[row])
            cell.set_fontsize(8)
            cell.set_edgecolor('#CCCCCC')
            cell.set_linewidth(0.5)
            
            # æ ‡é¢˜è¡ŒåŠ ç²—ï¼ˆè“è‰²èƒŒæ™¯çš„è¡Œï¼‰
            if row_colors[row] == '#4A90E2':
                cell.set_fontsize(9)
                # è®¾ç½®æ–‡æœ¬å±æ€§
                text = cell.get_text()
                text.set_fontweight('bold')
                text.set_color('white')
    
    # è°ƒæ•´è¡¨æ ¼ä½ç½®
    table.scale(1, 1.5)
    
    # ä¿å­˜å›¾è¡¨
    if save_path is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        save_path = TOOLS_OUTPUT_DIR / f"concept_analysis_{timestamp}.png"
    
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    logger.info(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
    return str(save_path)


# ==================== ä¸»å…¥å£å‡½æ•° ====================

def analyze_concepts(days: int = 5, market: str = "all",
                    include_kc: bool = False, include_cy: bool = False,
                    compress: bool = False,
                    save_chart: bool = False,
                    chart_path: Optional[Path] = None,
                    top_n: int = 20) -> Tuple[str, str]:
    """
    åˆ†æå¤§ç›˜æ¦‚å¿µè¶‹åŠ¿ï¼ˆä¸»å…¥å£å‡½æ•°ï¼‰
    
    Args:
        days: åˆ†æå¤©æ•°ï¼Œé»˜è®¤5å¤©
        market: å¸‚åœºç±»å‹ ('all' å…¨å¸‚åœº, 'sh' ä¸Šæµ·, 'sz' æ·±åœ³)
        include_kc: æ˜¯å¦åŒ…å«ç§‘åˆ›æ¿ï¼Œé»˜è®¤False
        include_cy: æ˜¯å¦åŒ…å«åˆ›ä¸šæ¿ï¼Œé»˜è®¤False
        compress: æ˜¯å¦å‹ç¼©JSONæ ¼å¼ï¼Œé»˜è®¤False
        save_chart: æ˜¯å¦ä¿å­˜å›¾è¡¨æ–‡ä»¶ï¼Œé»˜è®¤False
        chart_path: å›¾è¡¨ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        top_n: æ˜¾ç¤ºå‰Nä¸ªæ¦‚å¿µï¼Œé»˜è®¤20ï¼ˆä»…å½±å“å›¾è¡¨æ˜¾ç¤ºï¼Œä¸å½±å“JSONè¾“å‡ºï¼‰
        
    Returns:
        (JSONæ ¼å¼çš„å­—ç¬¦ä¸²ç»“æœ, å›¾è¡¨æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²)
        
    Note:
        JSONè¾“å‡ºåªåŒ…å«Top 10æ¦‚å¿µçš„åŸºæœ¬ä¿¡æ¯ï¼š
        - concept_name: æ¦‚å¿µåç§°
        - total_avg_change: æœ€è¿‘5ä¸ªäº¤æ˜“æ—¥ç´¯è®¡æ¶¨è·Œå¹…
        - total_up_count: å¯¹åº”ä¸Šæ¶¨è‚¡ç¥¨æ•°é‡
        - total_down_count: å¯¹åº”ä¸‹è·Œè‚¡ç¥¨æ•°é‡
    """
    try:
        trading_days = _get_trading_days(days + 7)
        analysis_days = trading_days[:days]  # å–æœ€æ–°çš„daysä¸ªäº¤æ˜“æ—¥
        
        logger.info(f"åˆ†ææ—¥æœŸ: {analysis_days}")
        
        # åŠ è½½åŸºç¡€æ•°æ®
        stock_list_data = JsonUtil.load(STOCK_LIST_FILE)
        if not stock_list_data:
            return JsonUtil.dumps({"message": "è‚¡ç¥¨åˆ—è¡¨æ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆæ›´æ–°æ•°æ®"}), "æ•°æ®ä¸å­˜åœ¨"
        
        stock_list = pd.DataFrame(stock_list_data)
        if stock_list.empty:
            return JsonUtil.dumps({"message": "è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º"}), "è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º"
        
        # å¸‚åœºç­›é€‰
        if market == "sh":
            stock_list = stock_list[stock_list['code'].str.startswith('6')]
        elif market == "sz":
            stock_list = stock_list[stock_list['code'].str.startswith(('0', '3'))]
        
        # æ¿å—ç­›é€‰
        if not include_kc:
            stock_list = stock_list[~stock_list['code'].str.startswith('68')]
        if not include_cy:
            stock_list = stock_list[~stock_list['code'].str.startswith('3')]
        
        # åŠ è½½æ¦‚å¿µæ˜ å°„
        concept_map = JsonUtil.load(CONCEPT_MAP_FILE) or {}
        
        # æ„å»ºæ¦‚å¿µåˆ°è‚¡ç¥¨çš„æ˜ å°„
        concept_to_stocks = defaultdict(list)
        for code, concepts in concept_map.items():
            filtered_concepts = _filter_concepts(concepts)
            for concept in filtered_concepts:
                concept_to_stocks[concept].append(code)
        
        # è¿‡æ»¤æ‰è‚¡ç¥¨åˆ—è¡¨ä¸­æ²¡æœ‰çš„è‚¡ç¥¨
        valid_codes = set(stock_list['code'].tolist())
        for concept in list(concept_to_stocks.keys()):
            concept_to_stocks[concept] = [
                code for code in concept_to_stocks[concept] 
                if code in valid_codes
            ]
        
        logger.info(f"åˆ†æ {len(concept_to_stocks)} ä¸ªæ¦‚å¿µ...")
        
        # åˆ†ææ¯ä¸ªæ¦‚å¿µ
        concept_results = []
        for concept_name, stock_codes in tqdm(concept_to_stocks.items(), desc="åˆ†ææ¦‚å¿µ", unit="ä¸ª"):
            result = _analyze_concept(
                concept_name, stock_codes, stock_list, analysis_days, concept_map
            )
            if result:
                concept_results.append(result)
        
        logger.info(f"æ‰¾åˆ° {len(concept_results)} ä¸ªæœ‰æ•ˆæ¦‚å¿µ")
        
        # æ ¼å¼åŒ–JSONç»“æœï¼ˆç®€åŒ–ç‰ˆï¼Œåªè¿”å›Top 10åŸºæœ¬ä¿¡æ¯ï¼‰
        result_json = _format_concept_result(concept_results, analysis_days, compress=compress)
        result_dict = JsonUtil.loads(result_json) or {}
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_output_path = None
        if result_dict:
            json_filename = f"concept_analysis_{datetime.now().strftime('%Y%m%d')}_{days}days_{market}_kc{include_kc}_cy{include_cy}.json"
            json_output_path = TOOLS_OUTPUT_DIR / json_filename
            JsonUtil.save(result_dict, json_output_path)
            result_dict["json_output_path"] = str(json_output_path)
        
        # ç”Ÿæˆå›¾è¡¨ï¼ˆä½¿ç”¨å®Œæ•´æ•°æ®ï¼‰
        chart_output_path = None
        if save_chart and concept_results:
            if chart_path is None:
                chart_path = TOOLS_OUTPUT_DIR / f"concept_analysis_{datetime.now().strftime('%Y%m%d')}_{days}days_{market}_kc{include_kc}_cy{include_cy}.png"
            
            # ä¸ºå›¾è¡¨ç”Ÿæˆå‡†å¤‡å®Œæ•´æ•°æ®
            chart_data = _format_concept_result_for_chart(concept_results, analysis_days)
            chart_result_dict = JsonUtil.loads(chart_data) or {}
            chart_output_path = generate_chart_from_results(chart_result_dict, save_path=chart_path, top_n=top_n)
            result_dict["chart_output_path"] = chart_output_path
        else:
            chart_output_path = "æœªæ‰¾åˆ°æ¦‚å¿µæ•°æ®"
        
        return JsonUtil.dumps(result_dict), chart_output_path
        
    except Exception as e:
        logger.error(f"åˆ†ææ¦‚å¿µæ•°æ®å¤±è´¥: {e}")
        error_result = JsonUtil.dumps({"message": f"åˆ†æå¤±è´¥: {str(e)}"})
        return error_result, "åˆ†æå¤±è´¥"


# ==================== å…¼å®¹æ€§å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰====================

def analyze_concepts_simple(days: int = 5, market: str = "all",
                           include_kc: bool = False, include_cy: bool = False,
                           compress: bool = False) -> str:
    """
    åˆ†æå¤§ç›˜æ¦‚å¿µè¶‹åŠ¿ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼Œä»…è¿”å›JSONï¼‰
    
    Args:
        days: åˆ†æå¤©æ•°ï¼Œé»˜è®¤5å¤©
        market: å¸‚åœºç±»å‹ ('all' å…¨å¸‚åœº, 'sh' ä¸Šæµ·, 'sz' æ·±åœ³)
        include_kc: æ˜¯å¦åŒ…å«ç§‘åˆ›æ¿ï¼Œé»˜è®¤False
        include_cy: æ˜¯å¦åŒ…å«åˆ›ä¸šæ¿ï¼Œé»˜è®¤False
        compress: æ˜¯å¦å‹ç¼©JSONæ ¼å¼ï¼Œé»˜è®¤False
        
    Returns:
        JSONæ ¼å¼çš„å­—ç¬¦ä¸²ç»“æœ
    """
    result_json, _ = analyze_concepts(
        days=days, market=market,
        include_kc=include_kc, include_cy=include_cy,
        compress=compress
    )
    return result_json


# ==================== MCP/Tool åŒ…è£… ====================

def get_mcp_tool() -> Dict:
    """
    è·å–MCPå·¥å…·å®šä¹‰
    
    Returns:
        MCPå·¥å…·å®šä¹‰å­—å…¸
    """
    return {
        "name": "analyze_concepts",
        "description": "åˆ†æå¤§ç›˜æ¦‚å¿µè¶‹åŠ¿ï¼ˆåŸºäºæœ¬åœ°ç¼“å­˜æ•°æ®ï¼‰",
        "inputSchema": {
            "type": "object",
            "properties": {
                "days": {
                    "type": "integer",
                    "description": "åˆ†æå¤©æ•°ï¼Œé»˜è®¤5å¤©",
                    "default": 5
                },
                "market": {
                    "type": "string",
                    "description": "å¸‚åœºç±»å‹ï¼š'all'å…¨å¸‚åœºï¼Œ'sh'ä¸Šæµ·ï¼Œ'sz'æ·±åœ³",
                    "enum": ["all", "sh", "sz"],
                    "default": "all"
                },
                "include_kc": {
                    "type": "boolean",
                    "description": "æ˜¯å¦åŒ…å«ç§‘åˆ›æ¿ï¼Œé»˜è®¤False",
                    "default": False
                },
                "include_cy": {
                    "type": "boolean",
                    "description": "æ˜¯å¦åŒ…å«åˆ›ä¸šæ¿ï¼Œé»˜è®¤False",
                    "default": False
                },
                "compress": {
                    "type": "boolean",
                    "description": "æ˜¯å¦å‹ç¼©JSONæ ¼å¼ï¼ˆå‡å°‘tokenï¼‰ï¼Œé»˜è®¤False",
                    "default": False
                }
            },
            "required": []
        }
    }


def handle_mcp_call(arguments: Dict) -> Dict:
    """
    å¤„ç†MCPå·¥å…·è°ƒç”¨
    
    Args:
        arguments: å·¥å…·å‚æ•°
        
    Returns:
        å·¥å…·æ‰§è¡Œç»“æœ
    """
    try:
        days = arguments.get("days", 5)
        market = arguments.get("market", "all")
        include_kc = arguments.get("include_kc", False)
        include_cy = arguments.get("include_cy", False)
        compress = arguments.get("compress", False)
        
        result_json, _ = analyze_concepts(
            days=days, market=market,
            include_kc=include_kc, include_cy=include_cy,
            compress=compress
        )
        
        return {
            "content": [
                {
                    "type": "text",
                    "text": result_json
                }
            ],
            "isError": False
        }
    except Exception as e:
        logger.error(f"MCPè°ƒç”¨å¤±è´¥: {e}")
        return {
            "content": [
                {
                    "type": "text",
                    "text": JsonUtil.dumps({"error": str(e)})
                }
            ],
            "isError": True
        }


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•° - CLIå…¥å£"""
    try:
        print("=" * 70)
        print("å¤§ç›˜æ¦‚å¿µè¶‹åŠ¿åˆ†æ - å¼€å§‹åˆ†æ")
        print("=" * 70)
        
        result_json, chart_content = analyze_concepts(
            days=5,
            market="all",
            include_kc=False,
            include_cy=False,
            compress=False,
            save_chart=True,
            top_n=10
        )
        
        result = JsonUtil.loads(result_json) or {}
        
        # æ‰“å°æŸ¥è¯¢ç»“æœæ¶ˆæ¯
        if result.get("concepts"):
            print(f"\nâœ… åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {len(result.get('concepts', []))} ä¸ªçƒ­é—¨æ¦‚å¿µ")
            print(f"ğŸ“Š åˆ†æå‘¨æœŸ: {result.get('analysis_days', 5)} ä¸ªäº¤æ˜“æ—¥")
            print(f"â° æŸ¥è¯¢æ—¶é—´: {result.get('query_time', 'N/A')}")
            print(f"ğŸ“ˆ æ€»æ¦‚å¿µæ•°: {result.get('total_concepts', 0)}")
            
            # æ‰“å°å‰10ä¸ªæ¦‚å¿µ
            print("\nğŸ”¥ çƒ­é—¨æ¦‚å¿µTOP10:")
            for i, concept in enumerate(result.get('concepts', [])[:10], 1):
                print(f"   {i}. {concept.get('concept_name', 'N/A')} - "
                      f"{concept.get('total_avg_change', '0%')} "
                      f"(ä¸Šæ¶¨:{concept.get('total_up_count', 0)} "
                      f"ä¸‹è·Œ:{concept.get('total_down_count', 0)})")
            
            # æ‰“å°å›¾è¡¨æ–‡ä»¶è·¯å¾„
            print(f"\nğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_content}")
            
            # è¾“å‡ºJSONç»“æœåˆ°æ–‡ä»¶
            json_output_path = TOOLS_OUTPUT_DIR / f"concept_analysis_{datetime.now().strftime('%Y%m%d')}.json"
            print(f"ğŸ“„ å‡†å¤‡ä¿å­˜JSONç»“æœåˆ°: {json_output_path}")
            print(f"ğŸ“„ resultç±»å‹: {type(result)}, æ˜¯å¦ä¸ºç©º: {not result}")
            
            save_success = JsonUtil.save(result, json_output_path)
            if save_success:
                print(f"ğŸ“„ JSONç»“æœå·²ä¿å­˜åˆ°: {json_output_path}")
                print(f"ğŸ“„ æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {json_output_path.exists()}")
            else:
                print(f"âŒ JSONä¿å­˜å¤±è´¥: {json_output_path}")
        else:
            print(f"\nâš ï¸  {result.get('message', 'æœªæ‰¾åˆ°æ¦‚å¿µæ•°æ®')}")
        
        print("\n" + "=" * 70)
        print("åˆ†æå®Œæˆ")
        print("=" * 70)
        
    except Exception as e:
        logger.error(f"å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()

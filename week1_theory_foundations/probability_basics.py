"""
æ¦‚ç‡è®ºåŸºç¡€ - å¤§æ¨¡å‹å·¥ç¨‹å¸ˆå¿…å¤‡æ•°å­¦çŸ¥è¯†

æœ¬æ¨¡å—æ¶µç›–å¤§æ¨¡å‹è®­ç»ƒä¸­å¸¸ç”¨çš„æ¦‚ç‡è®ºæ¦‚å¿µï¼Œé€šè¿‡ç›´è§‚çš„ä¾‹å­å’Œä»£ç æ¥ç†è§£ã€‚

æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š
1. æ¡ä»¶æ¦‚ç‡ä¸è´å¶æ–¯å®šç† - ç†è§£è¯­è¨€æ¨¡å‹çš„é¢„æµ‹
2. æœŸæœ›ä¸æ–¹å·® - ç†è§£æ¨¡å‹è¾“å‡ºçš„åˆ†å¸ƒ
3. å¸¸ç”¨åˆ†å¸ƒ - ç†è§£æ¨¡å‹çš„ä¸ç¡®å®šæ€§
4. Softmax å‡½æ•° - å°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats


# ============================================================
# 1. æ¡ä»¶æ¦‚ç‡ä¸è”åˆæ¦‚ç‡
# ============================================================

def explain_conditional_probability():
    """
    æ¡ä»¶æ¦‚ç‡ï¼šP(A|B) = P(Aâˆ©B) / P(B)
    
    åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼š
    - P(w_t | w_1, w_2, ..., w_{t-1})ï¼šç»™å®šå‰æ–‡ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡
    - è¿™æ˜¯è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒ
    
    ç¤ºä¾‹ï¼šå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„è¯æ±‡è¡¨ ['æˆ‘', 'çˆ±', 'å­¦ä¹ ', 'AI']
    æˆ‘ä»¬æƒ³è®¡ç®— P('AI' | 'æˆ‘', 'çˆ±', 'å­¦ä¹ ')
    """
    
    # ç¤ºä¾‹è¯æ±‡è¡¨å’Œç®€å•çš„è¯­è¨€æ¨¡å‹æ¦‚ç‡
    vocab = ['æˆ‘', 'çˆ±', 'å­¦ä¹ ', 'AI', 'ã€‚', 'å¾ˆ', 'æ£’']
    word_to_idx = {word: idx for idx, word in enumerate(vocab)}
    
    # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„è¯­è¨€æ¨¡å‹ï¼ˆå®é™…ä¸Šæ˜¯ä»æ•°æ®ä¸­å­¦ä¹ çš„ï¼‰
    # P(next_word | context)
    # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„è½¬ç§»æ¦‚ç‡çŸ©é˜µ
    transition_matrix = np.array([
        [0.0, 0.3, 0.4, 0.1, 0.1, 0.1, 0.0],  # 'æˆ‘' åé¢å¯èƒ½çš„è¯
        [0.0, 0.0, 0.3, 0.5, 0.1, 0.1, 0.0],  # 'çˆ±' åé¢å¯èƒ½çš„è¯
        [0.0, 0.0, 0.0, 0.6, 0.2, 0.1, 0.1],  # 'å­¦ä¹ ' åé¢å¯èƒ½çš„è¯
        [0.1, 0.0, 0.0, 0.0, 0.3, 0.3, 0.3],  # 'AI' åé¢å¯èƒ½çš„è¯
        [0.3, 0.2, 0.1, 0.1, 0.0, 0.1, 0.2],  # 'ã€‚' åé¢å¯èƒ½çš„è¯
        [0.2, 0.3, 0.2, 0.2, 0.0, 0.0, 0.1],  # 'å¾ˆ' åé¢å¯èƒ½çš„è¯
        [0.1, 0.2, 0.2, 0.3, 0.2, 0.1, 0.0],  # 'æ£’' åé¢å¯èƒ½çš„è¯
    ])
    
    # ç¤ºä¾‹ï¼šç»™å®š "æˆ‘ çˆ± å­¦ä¹ "ï¼Œè®¡ç®—ä¸‹ä¸€ä¸ªè¯æ˜¯ "AI" çš„æ¦‚ç‡
    context = ['æˆ‘', 'çˆ±', 'å­¦ä¹ ']
    context_indices = [word_to_idx[w] for w in context]
    
    # åœ¨å®é™…è¯­è¨€æ¨¡å‹ä¸­ï¼Œä¼šè€ƒè™‘æ›´é•¿çš„ä¸Šä¸‹æ–‡
    # è¿™é‡Œç®€åŒ–ï¼šä½¿ç”¨æœ€åä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒ
    last_word_idx = context_indices[-1]
    next_word_probs = transition_matrix[last_word_idx]
    
    print("=" * 60)
    print("æ¡ä»¶æ¦‚ç‡ç¤ºä¾‹ï¼šè¯­è¨€æ¨¡å‹é¢„æµ‹")
    print("=" * 60)
    print(f"ä¸Šä¸‹æ–‡: {' '.join(context)}")
    print(f"æœ€åè¯ç´¢å¼•: {last_word_idx} ('{context[-1]}')")
    print("\né¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒ:")
    for i, prob in enumerate(next_word_probs):
        if prob > 0.01:  # åªæ˜¾ç¤ºæ¦‚ç‡å¤§äº 1% çš„è¯
            print(f"  P('{vocab[i]}' | '{context[-1]}') = {prob:.3f}")
    
    # è®¡ç®— P('AI' | 'å­¦ä¹ ') = 0.6
    ai_prob = next_word_probs[word_to_idx['AI']]
    print(f"\nâ†’ é¢„æµ‹ 'AI' çš„æ¦‚ç‡: {ai_prob:.3f}")
    print()


def explain_joint_probability():
    """
    è”åˆæ¦‚ç‡ï¼šP(Aâˆ©B) = P(A) * P(B|A)
    
    åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼š
    - P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2|w_1) * P(w_3|w_1,w_2) * ...
    - è¿™æ˜¯å¥å­æ¦‚ç‡è®¡ç®—çš„åŸºç¡€
    
    ç¤ºä¾‹ï¼šè®¡ç®—å¥å­ "æˆ‘çˆ±å­¦ä¹  AI" çš„æ¦‚ç‡
    """
    
    vocab = ['æˆ‘', 'çˆ±', 'å­¦ä¹ ', 'AI', 'ã€‚']
    word_to_idx = {word: idx for idx, word in enumerate(vocab)}
    
    # ç®€åŒ–çš„è½¬ç§»æ¦‚ç‡çŸ©é˜µ
    transition_matrix = np.array([
        [0.0, 0.3, 0.4, 0.1, 0.1],  # 'æˆ‘'
        [0.0, 0.0, 0.3, 0.5, 0.1],  # 'çˆ±'
        [0.0, 0.0, 0.0, 0.6, 0.2],  # 'å­¦ä¹ '
        [0.0, 0.0, 0.0, 0.0, 0.3],  # 'AI'
        [0.2, 0.2, 0.1, 0.2, 0.0],  # 'ã€‚'
    ])
    
    # åˆå§‹æ¦‚ç‡ P(w_1)
    start_probs = np.array([0.2, 0.1, 0.3, 0.3, 0.1])
    
    # å¥å­ "æˆ‘çˆ±å­¦ä¹  AI" = w_1=æˆ‘, w_2=çˆ±, w_3=å­¦ä¹ , w_4=AI, w_5=ã€‚
    sentence = ['æˆ‘', 'çˆ±', 'å­¦ä¹ ', 'AI', 'ã€‚']
    
    # è®¡ç®—è”åˆæ¦‚ç‡ P(w_1, w_2, w_3, w_4, w_5)
    prob = start_probs[word_to_idx['æˆ‘']]  # P(æˆ‘)
    prob *= transition_matrix[word_to_idx['æˆ‘']][word_to_idx['çˆ±']]  # P(çˆ±|æˆ‘)
    prob *= transition_matrix[word_to_idx['çˆ±']][word_to_idx['å­¦ä¹ ']]  # P(å­¦ä¹ |æˆ‘çˆ±)
    prob *= transition_matrix[word_to_idx['å­¦ä¹ ']][word_to_idx['AI']]  # P(AI|æˆ‘çˆ±å­¦ä¹ )
    prob *= transition_matrix[word_to_idx['AI']][word_to_idx['ã€‚']]  # P(ã€‚|æˆ‘çˆ±å­¦ä¹ AI)
    
    print("=" * 60)
    print("è”åˆæ¦‚ç‡ç¤ºä¾‹ï¼šå¥å­æ¦‚ç‡è®¡ç®—")
    print("=" * 60)
    print(f"å¥å­: {' '.join(sentence)}")
    print(f"\næ¦‚ç‡åˆ†è§£:")
    print(f"  P(æˆ‘)           = {start_probs[word_to_idx['æˆ‘']]:.3f}")
    print(f"  P(çˆ±|æˆ‘)        = {transition_matrix[word_to_idx['æˆ‘']][word_to_idx['çˆ±']]:.3f}")
    print(f"  P(å­¦ä¹ |æˆ‘çˆ±)    = {transition_matrix[word_to_idx['çˆ±']][word_to_idx['å­¦ä¹ ']]:.3f}")
    print(f"  P(AI|æˆ‘çˆ±å­¦ä¹ )  = {transition_matrix[word_to_idx['å­¦ä¹ ']][word_to_idx['AI']]:.3f}")
    print(f"  P(ã€‚|æˆ‘çˆ±å­¦ä¹ AI)= {transition_matrix[word_to_idx['AI']][word_to_idx['ã€‚']]:.3f}")
    print(f"\n  è”åˆæ¦‚ç‡ P(æˆ‘,çˆ±,å­¦ä¹ ,AI,ã€‚) = {prob:.6f}")
    print()


# ============================================================
# 2. æœŸæœ›ä¸æ–¹å·®
# ============================================================

def explain_expectation_variance():
    """
    æœŸæœ› E[X] = Î£ x * P(x)
    æ–¹å·® Var(X) = E[(X - E[X])^2]
    
    åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼š
    - æœŸæœ›ï¼šé¢„æµ‹è¯çš„æ¦‚ç‡åˆ†å¸ƒçš„"å¹³å‡å€¼"
    - æ–¹å·®ï¼šé¢„æµ‹çš„ä¸ç¡®å®šæ€§åº¦é‡
    
    ç¤ºä¾‹ï¼šåˆ†æè¯­è¨€æ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§
    """
    
    # æ¨¡æ‹Ÿä¸€ä¸ªè¯­è¨€æ¨¡å‹å¯¹ä¸‹ä¸€ä¸ªè¯çš„é¢„æµ‹åˆ†å¸ƒ
    vocab = ['æˆ‘', 'çˆ±', 'å­¦ä¹ ', 'AI', 'ç¼–ç¨‹', 'å¾ˆ', 'æ£’', 'ã€‚']
    
    # å‡è®¾æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒ
    probs = np.array([0.05, 0.10, 0.15, 0.30, 0.20, 0.05, 0.10, 0.05])
    
    # æœŸæœ›ï¼ˆç†µæœ€å¤§æ—¶çš„å¹³å‡é¢„æµ‹ä½ç½®ï¼‰
    expectation = np.sum(np.arange(len(vocab)) * probs)
    
    # æ–¹å·®ï¼ˆé¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼‰
    variance = np.sum(((np.arange(len(vocab)) - expectation) ** 2) * probs)
    
    # ç†µï¼ˆä¿¡æ¯é‡çš„æœŸæœ›ï¼‰
    entropy = -np.sum(probs * np.log2(probs + 1e-10))
    
    print("=" * 60)
    print("æœŸæœ›ä¸æ–¹å·®ç¤ºä¾‹ï¼šæ¨¡å‹é¢„æµ‹åˆ†æ")
    print("=" * 60)
    print(f"è¯æ±‡è¡¨: {vocab}")
    print(f"\né¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ:")
    for i, (word, prob) in enumerate(zip(vocab, probs)):
        bar = 'â–ˆ' * int(prob * 50)
        print(f"  {word}: {prob:.2f} {bar}")
    
    print(f"\nç»Ÿè®¡é‡:")
    print(f"  æœŸæœ› E[X] = {expectation:.2f}")
    print(f"  æ–¹å·® Var(X) = {variance:.2f}")
    print(f"  ç†µ H(X) = {entropy:.2f} bits")
    print(f"\nè§£è¯»:")
    print(f"  - æœŸæœ› 2.65 è¡¨ç¤ºé¢„æµ‹ä¸»è¦åˆ†å¸ƒåœ¨ 'å­¦ä¹ ' é™„è¿‘")
    print(f"  - æ–¹å·® 3.21 è¡¨ç¤ºé¢„æµ‹æœ‰ä¸€å®šçš„ä¸ç¡®å®šæ€§")
    print(f"  - ç†µ 2.45 bits è¡¨ç¤ºå¹³å‡éœ€è¦ 2.45 bits æ¥ç¼–ç è¿™ä¸ªé¢„æµ‹")
    print()


# ============================================================
# 3. Softmax å‡½æ•°
# ============================================================

def explain_softmax():
    """
    Softmax å‡½æ•°ï¼šå°† logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    
    å…¬å¼ï¼šsoftmax(x_i) = exp(x_i) / Î£ exp(x_j)
    
    åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼š
    - å°†æ¨¡å‹çš„åŸå§‹è¾“å‡ºï¼ˆlogitsï¼‰è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    - ä¿è¯æ‰€æœ‰æ¦‚ç‡ä¹‹å’Œä¸º 1
    - æ¸©åº¦å‚æ•° T æ§åˆ¶åˆ†å¸ƒçš„"é”åˆ©"ç¨‹åº¦
    
    softmax(x_i, T) = exp(x_i/T) / Î£ exp(x_j/T)
    
    - T â†’ 0ï¼šè¶‹è¿‘äº one-hot åˆ†å¸ƒï¼ˆæœ€ç¡®å®šçš„é¢„æµ‹ï¼‰
    - T â†’ 1ï¼šæ­£å¸¸ softmax
    - T â†’ âˆï¼šè¶‹è¿‘äºå‡åŒ€åˆ†å¸ƒï¼ˆæœ€ä¸ç¡®å®šçš„é¢„æµ‹ï¼‰
    """
    
    # å‡è®¾æ¨¡å‹è¾“å‡ºçš„åŸå§‹ logits
    logits = np.array([2.0, 1.0, 0.1, 3.0, 1.5])
    
    # æ ‡å‡† softmax
    exp_logits = np.exp(logits)
    probs = exp_logits / np.sum(exp_logits)
    
    # ä¸åŒæ¸©åº¦ä¸‹çš„ softmax
    def softmax_with_temp(logits, T):
        exp_logits = np.exp(logits / T)
        return exp_logits / np.sum(exp_logits)
    
    temps = [0.1, 0.5, 1.0, 2.0, 5.0]
    
    print("=" * 60)
    print("Softmax å‡½æ•°è¯¦è§£")
    print("=" * 60)
    print(f"åŸå§‹ logits: {logits}")
    print(f"\næ ‡å‡† softmax (T=1.0):")
    for i, p in enumerate(probs):
        bar = 'â–ˆ' * int(p * 50)
        print(f"  logit[{i}]={logits[i]:.1f} â†’ prob={p:.4f} {bar}")
    
    print(f"\nä¸åŒæ¸©åº¦ä¸‹çš„æ¦‚ç‡åˆ†å¸ƒ:")
    print(f"{'Temp':<8} {'åˆ†å¸ƒ (æ¦‚ç‡å€¼)':<60}")
    print("-" * 70)
    
    for T in temps:
        probs_T = softmax_with_temp(logits, T)
        dist_str = '[' + ', '.join([f'{p:.3f}' for p in probs_T]) + ']'
        if T <= 1.0:
            dist_str += ' ğŸ”¥'  # æ›´ç¡®å®š
        else:
            dist_str += ' ğŸ²'  # æ›´éšæœº
        print(f"{T:<8} {dist_str:<60}")
    
    print()
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # å›¾1ï¼šä¸åŒæ¸©åº¦ä¸‹çš„æ¦‚ç‡åˆ†å¸ƒ
    x = np.arange(len(logits))
    for T in [0.1, 0.5, 1.0, 2.0]:
        probs_T = softmax_with_temp(logits, T)
        label = f'T={T}'
        if T == 0.1:
            label += ' (ğŸ”¥ç¡®å®š)'
        elif T == 2.0:
            label += ' (ğŸ²éšæœº)'
        axes[0].bar(x + (T-1)*0.15, probs_T, width=0.3, label=label, alpha=0.8)
    
    axes[0].set_xlabel('Token Index')
    axes[0].set_ylabel('Probability')
    axes[0].set_title('Softmax with Different Temperatures')
    axes[0].set_xticks(x)
    axes[0].legend()
    axes[0].grid(axis='y', alpha=0.3)
    
    # å›¾2ï¼šsoftmax çš„æ•°å­¦æ€§è´¨
    T_range = np.linspace(0.1, 5, 100)
    max_prob = [np.max(softmax_with_temp(logits, T)) for T in T_range]
    entropy = [-np.sum(softmax_with_temp(logits, T) * 
              np.log2(softmax_with_temp(logits, T) + 1e-10)) for T in T_range]
    
    axes[1].plot(T_range, max_prob, 'b-', linewidth=2, label='Max Probability')
    axes[1].set_xlabel('Temperature')
    axes[1].set_ylabel('Value', color='b')
    axes[1].tick_params(axis='y', labelcolor='b')
    
    ax2 = axes[1].twinx()
    ax2.plot(T_range, entropy, 'r-', linewidth=2, label='Entropy (bits)')
    ax2.set_ylabel('Entropy (bits)', color='r')
    ax2.tick_params(axis='y', labelcolor='r')
    
    axes[1].set_title('Temperature vs. Certainty & Entropy')
    axes[1].legend(loc='upper left')
    ax2.legend(loc='upper right')
    axes[1].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('week1_theory_foundations/softmax_temperature_analysis.png', dpi=150, bbox_inches='tight')
    print(f"å›¾ç‰‡å·²ä¿å­˜åˆ°: week1_theory_foundations/softmax_temperature_analysis.png")
    print()


# ============================================================
# 4. äº¤å‰ç†µä¸ KL æ•£åº¦
# ============================================================

def explain_cross_entropy_kl():
    """
    äº¤å‰ç†µï¼šH(P, Q) = -Î£ P(x) log Q(x)
    
    KL æ•£åº¦ï¼ˆç›¸å¯¹ç†µï¼‰ï¼šD_KL(P || Q) = Î£ P(x) log(P(x)/Q(x))
    
    å…³ç³»ï¼šH(P, Q) = H(P) + D_KL(P || Q)
    
    åœ¨è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼š
    - äº¤å‰ç†µæŸå¤± = -Î£ y_true * log(y_pred)
    - æœ€å°åŒ–äº¤å‰ç†µç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ï¼ˆå½“ H(P) å›ºå®šæ—¶ï¼‰
    - è®©æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒæ¥è¿‘çœŸå®åˆ†å¸ƒ
    """
    
    # ç¤ºä¾‹ï¼šè®¡ç®—äº¤å‰ç†µæŸå¤±
    # å‡è®¾çœŸå®æ ‡ç­¾æ˜¯ one-hot ç¼–ç  [0, 0, 1, 0, 0]ï¼ˆç¬¬3ä¸ªè¯æ˜¯æ­£ç¡®ç­”æ¡ˆï¼‰
    y_true_onehot = np.array([0, 0, 1, 0, 0])
    
    # æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒ
    y_pred = np.array([0.05, 0.10, 0.70, 0.10, 0.05])
    
    # äº¤å‰ç†µæŸå¤±ï¼ˆç”¨äºåˆ†ç±»ä»»åŠ¡ï¼‰
    # å¯¹äº one-hot æ ‡ç­¾ï¼Œäº¤å‰ç†µ = -log(P(true_class))
    cross_entropy = -np.sum(y_true_onehot * np.log(y_pred + 1e-10))
    
    # é€å…ƒç´ è®¡ç®—
    print("=" * 60)
    print("äº¤å‰ç†µä¸ KL æ•£åº¦è¯¦è§£")
    print("=" * 60)
    print(f"çœŸå®åˆ†å¸ƒ (one-hot): {y_true_onehot}")
    print(f"é¢„æµ‹åˆ†å¸ƒ:           {y_pred}")
    print(f"\näº¤å‰ç†µè®¡ç®—è¿‡ç¨‹:")
    print(f"  H(P, Q) = -Î£ P(x)Â·log(Q(x))")
    print(f"  = -[0Â·log(0.05) + 0Â·log(0.10) + 1Â·log(0.70) + 0Â·log(0.10) + 0Â·log(0.05)]")
    print(f"  = -{np.log(0.70):.4f}")
    print(f"  = {cross_entropy:.4f}")
    
    # å¯¹æ•°ä¼¼ç„¶ï¼ˆä»¥ 2 ä¸ºåº•ï¼Œå•ä½æ˜¯ bitsï¼‰
    log_likelihood_2 = np.log2(0.70)
    print(f"\nå¯¹æ•°ä¼¼ç„¶ (log2): {log_likelihood_2:.4f} bits")
    print(f"å›°æƒ‘åº¦ (Perplexity): {2 ** cross_entropy:.2f}")
    
    # ä¸åŒé¢„æµ‹è´¨é‡çš„å¯¹æ¯”
    print(f"\nä¸åŒé¢„æµ‹è´¨é‡çš„äº¤å‰ç†µæŸå¤±å¯¹æ¯”:")
    predictions = [
        ([0.05, 0.10, 0.70, 0.10, 0.05], "é«˜è´¨é‡é¢„æµ‹"),
        ([0.20, 0.20, 0.30, 0.20, 0.10], "ä¸­ç­‰è´¨é‡é¢„æµ‹"),
        ([0.05, 0.05, 0.05, 0.05, 0.80], "é”™è¯¯é¢„æµ‹ï¼ˆé«˜æŸå¤±ï¼‰"),
    ]
    
    print(f"{'é¢„æµ‹åˆ†å¸ƒ':<45} {'äº¤å‰ç†µæŸå¤±':<12} {'å›°æƒ‘åº¦':<10}")
    print("-" * 70)
    
    for pred, desc in predictions:
        pred = np.array(pred)
        ce = -np.sum(y_true_onehot * np.log(pred + 1e-10))
        ppl = 2 ** ce
        pred_str = '[' + ', '.join([f'{p:.2f}' for p in pred]) + ']'
        print(f"{pred_str:<45} {ce:<12.4f} {ppl:<10.2f} {desc}")
    
    print()
    print("å…³é”®ç†è§£:")
    print("  - äº¤å‰ç†µæŸå¤±è¶Šä½ï¼Œé¢„æµ‹è´¨é‡è¶Šå¥½")
    print("  - å›°æƒ‘åº¦æ˜¯äº¤å‰ç†µçš„æŒ‡æ•°ï¼Œå¯ä»¥ç†è§£ä¸ºå¹³å‡å€™é€‰è¯æ•°")
    print("  - å›°æƒ‘åº¦ = 2^äº¤å‰ç†µï¼ˆä»¥ 2 ä¸ºåº•ï¼‰")
    print()


# ============================================================
# 5. ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹
# ============================================================

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("æ¦‚ç‡è®ºåŸºç¡€ - å¤§æ¨¡å‹å·¥ç¨‹å¸ˆå¿…å¤‡æ•°å­¦çŸ¥è¯†")
    print("=" * 70 + "\n")
    
    # è¿è¡Œå„ä¸ªç¤ºä¾‹
    explain_conditional_probability()
    explain_joint_probability()
    explain_expectation_variance()
    explain_softmax()
    explain_cross_entropy_kl()
    
    print("=" * 70)
    print("å­¦ä¹ æ€»ç»“")
    print("=" * 70)
    print("""
æ ¸å¿ƒæ¦‚å¿µå›é¡¾ï¼š

1. æ¡ä»¶æ¦‚ç‡ P(A|B)
   - è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒï¼šP(next_word | context)
   
2. è”åˆæ¦‚ç‡ P(Aâˆ©B)
   - å¥å­æ¦‚ç‡ï¼šP(w_1, w_2, ..., w_n) = Î  P(w_i | w_1...w_{i-1})
   
3. æœŸæœ›ä¸æ–¹å·®
   - æœŸæœ›ï¼šé¢„æµ‹åˆ†å¸ƒçš„"é‡å¿ƒ"
   - æ–¹å·®ï¼šé¢„æµ‹çš„ä¸ç¡®å®šæ€§
   
4. Softmax å‡½æ•°
   - å°† logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
   - æ¸©åº¦å‚æ•°æ§åˆ¶åˆ†å¸ƒçš„"é”åˆ©"ç¨‹åº¦
   
5. äº¤å‰ç†µæŸå¤±
   - è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒæŸå¤±å‡½æ•°
   - å›°æƒ‘åº¦æ˜¯è¯„ä¼°è¯­è¨€æ¨¡å‹çš„é‡è¦æŒ‡æ ‡

ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š
- çº¿æ€§ä»£æ•°ï¼šçŸ©é˜µè¿ç®—ã€ç‰¹å¾åˆ†è§£
- ä¼˜åŒ–ç®—æ³•ï¼šæ¢¯åº¦ä¸‹é™ã€Adam
- æ³¨æ„åŠ›æœºåˆ¶ï¼šQuery-Key-Value æŠ½è±¡
    """)

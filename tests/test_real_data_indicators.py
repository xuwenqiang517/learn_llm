"""
çœŸå®æ•°æ®æ–‡ä»¶æŠ€æœ¯æŒ‡æ ‡éªŒè¯æµ‹è¯•

åŸºäº load_base_data.py äº§å‡ºçš„çœŸå®æ•°æ®æ–‡ä»¶
éªŒè¯æ‰€æœ‰è®¡ç®—åŠ å·¥å­—æ®µçš„å‡†ç¡®æ€§ï¼Œå‘ç°å¹¶ä¿®å¤é€»è¾‘æ¼æ´
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
import os

sys.path.insert(0, '/Users/wq/Documents/github/learn_llm')

from utils.data_path_util import get_stock_data_dir, get_etf_data_dir


def get_sample_files(n: int = 20) -> list:
    """è·å–æ ·æœ¬æ–‡ä»¶è¿›è¡Œæµ‹è¯•"""
    stock_dir = get_stock_data_dir()
    stock_files = list(stock_dir.glob("*.csv"))[:n]
    return stock_files


def manual_ma(close: pd.Series, window: int) -> pd.Series:
    """æ‰‹åŠ¨è®¡ç®—ç§»åŠ¨å¹³å‡çº¿"""
    return close.rolling(window=window).mean().round(2)


def manual_cumulative_returns(pct_change: pd.Series, window: int) -> pd.Series:
    """æ‰‹åŠ¨è®¡ç®—ç´¯è®¡æ¶¨å¹…"""
    return pct_change.rolling(window=window).sum().round(2)


def manual_consecutive_rise(pct_change: pd.Series) -> pd.Series:
    """æ‰‹åŠ¨è®¡ç®—è¿æ¶¨å¤©æ•°"""
    is_rising = (pct_change > 0).astype(int)
    consecutive = is_rising.cumsum() - is_rising.cumsum().where(is_rising == 0).ffill().fillna(0).astype(int)
    return consecutive


def verify_single_file(file_path: Path) -> dict:
    """éªŒè¯å•ä¸ªæ–‡ä»¶çš„æ‰€æœ‰è®¡ç®—å­—æ®µ"""
    result = {
        'file': file_path.name,
        'passed': True,
        'errors': []
    }
    
    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig', parse_dates=['æ—¥æœŸ'])
        if df.empty:
            result['passed'] = False
            result['errors'].append('æ–‡ä»¶ä¸ºç©º')
            return result
        
        if len(df) < 30:
            result['passed'] = False
            result['errors'].append(f'æ•°æ®è¡Œæ•°ä¸è¶³: {len(df)}')
            return result
        
        close = df['æ”¶ç›˜']
        pct_change = df['æ¶¨è·Œå¹…']
        high = df['æœ€é«˜']
        low = df['æœ€ä½']
        
        # 1. éªŒè¯MA5
        ma5_actual = df['MA5']
        ma5_expected = manual_ma(close, 5)
        ma5_diff = (ma5_actual.fillna(0) - ma5_expected.fillna(0)).abs().sum()
        if ma5_diff > 0.01:
            result['passed'] = False
            result['errors'].append(f'MA5å·®å¼‚: {ma5_diff:.2f}')
        
        # 2. éªŒè¯MA10
        ma10_actual = df['MA10']
        ma10_expected = manual_ma(close, 10)
        ma10_diff = (ma10_actual.fillna(0) - ma10_expected.fillna(0)).abs().sum()
        if ma10_diff > 0.01:
            result['passed'] = False
            result['errors'].append(f'MA10å·®å¼‚: {ma10_diff:.2f}')
        
        # 3. éªŒè¯MA20
        ma20_actual = df['MA20']
        ma20_expected = manual_ma(close, 20)
        ma20_diff = (ma20_actual.fillna(0) - ma20_expected.fillna(0)).abs().sum()
        if ma20_diff > 0.01:
            result['passed'] = False
            result['errors'].append(f'MA20å·®å¼‚: {ma20_diff:.2f}')
        
        # 4. éªŒè¯3æ—¥æ¶¨å¹…
        d3_actual = df['3æ—¥æ¶¨å¹…']
        d3_expected = manual_cumulative_returns(pct_change, 3)
        d3_diff = (d3_actual.fillna(0) - d3_expected.fillna(0)).abs().sum()
        if d3_diff > 0.01:
            result['passed'] = False
            result['errors'].append(f'3æ—¥æ¶¨å¹…å·®å¼‚: {d3_diff:.2f}')
        
        # 5. éªŒè¯5æ—¥æ¶¨å¹…
        d5_actual = df['5æ—¥æ¶¨å¹…']
        d5_expected = manual_cumulative_returns(pct_change, 5)
        d5_diff = (d5_actual.fillna(0) - d5_expected.fillna(0)).abs().sum()
        if d5_diff > 0.01:
            result['passed'] = False
            result['errors'].append(f'5æ—¥æ¶¨å¹…å·®å¼‚: {d5_diff:.2f}')
        
        # 6. éªŒè¯è¿æ¶¨å¤©æ•°
        consec_actual = df['è¿æ¶¨å¤©æ•°']
        consec_expected = manual_consecutive_rise(pct_change)
        match_count = (consec_actual.reset_index(drop=True) == consec_expected.reset_index(drop=True)).sum()
        total_count = len(consec_actual)
        if match_count != total_count:
            result['passed'] = False
            mismatch_rate = (total_count - match_count) / total_count * 100
            result['errors'].append(f'è¿æ¶¨å¤©æ•°ä¸åŒ¹é…: {total_count - match_count}/{total_count} ({mismatch_rate:.1f}%)')
            
            mismatched_indices = consec_actual[consec_actual != consec_expected].index.tolist()
            if mismatched_indices:
                sample_indices = mismatched_indices[:5]
                sample_info = []
                for idx in sample_indices:
                    if idx < len(pct_change):
                        sample_info.append(f'{idx}(æ¶¨è·Œå¹…={pct_change.iloc[idx]:.2f}%, å®é™…={consec_actual.iloc[idx]}, æœŸæœ›={consec_expected.iloc[idx]})')
                result['errors'].append(f'ç¤ºä¾‹: {sample_info}')
        
        # 7. éªŒè¯è¿æ¶¨å¤©æ•°é€»è¾‘æ­£ç¡®æ€§ï¼ˆæ¶¨äº†åº”è¯¥>0ï¼Œè·Œäº†åº”è¯¥=0ï¼‰
        for idx in range(len(df)):
            pct = pct_change.iloc[idx] if idx < len(pct_change) else 0
            consec = consec_actual.iloc[idx] if idx < len(consec_actual) else 0
            
            if pd.notna(pct) and pd.notna(consec):
                if pct > 0 and consec <= 0:
                    result['passed'] = False
                    result['errors'].append(f'è¿æ¶¨å¤©æ•°é€»è¾‘é”™è¯¯(æ¶¨äº†åº”è¯¥>0): idx={idx}, æ¶¨è·Œå¹…={pct:.2f}%, è¿æ¶¨å¤©æ•°={consec}')
                elif pct < 0 and consec != 0:
                    result['passed'] = False
                    result['errors'].append(f'è¿æ¶¨å¤©æ•°é€»è¾‘é”™è¯¯(è·Œäº†åº”è¯¥=0): idx={idx}, æ¶¨è·Œå¹…={pct:.2f}%, è¿æ¶¨å¤©æ•°={consec}')
        
    except Exception as e:
        result['passed'] = False
        result['errors'].append(f'å¼‚å¸¸: {str(e)}')
    
    return result


def verify_all_files(max_files: int = 50) -> dict:
    """éªŒè¯æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
    print("="*70)
    print("çœŸå®æ•°æ®æ–‡ä»¶æŠ€æœ¯æŒ‡æ ‡éªŒè¯")
    print("="*70)
    
    stock_dir = get_stock_data_dir()
    stock_files = list(stock_dir.glob("*.csv"))[:max_files]
    
    print(f"\néªŒè¯ {len(stock_files)} ä¸ªè‚¡ç¥¨æ•°æ®æ–‡ä»¶...")
    
    all_passed = True
    total_errors = 0
    error_summary = {}
    
    for file_path in stock_files:
        result = verify_single_file(file_path)
        if not result['passed']:
            all_passed = False
            total_errors += len(result['errors'])
            print(f"\nâŒ {result['file']}")
            for error in result['errors'][:3]:
                print(f"   {error}")
                if 'è¿æ¶¨å¤©æ•°' in error or 'MA' in error:
                    key = error.split('å·®å¼‚')[0].strip() if 'å·®å¼‚' in error else error.split('(')[0].strip()
                    error_summary[key] = error_summary.get(key, 0) + 1
    
    print("\n" + "="*70)
    print("éªŒè¯ç»“æœæ±‡æ€»")
    print("="*70)
    print(f"éªŒè¯æ–‡ä»¶æ•°: {len(stock_files)}")
    print(f"å…¨éƒ¨é€šè¿‡: {'æ˜¯' if all_passed else 'å¦'}")
    
    if error_summary:
        print("\né”™è¯¯ç±»å‹ç»Ÿè®¡:")
        for error_type, count in sorted(error_summary.items(), key=lambda x: -x[1]):
            print(f"  {error_type}: {count}æ¬¡")
    
    return {
        'all_passed': all_passed,
        'total_errors': total_errors,
        'error_summary': error_summary
    }


def find_consecutive_rise_bugs(max_files: int = 50) -> list:
    """ä¸“é—¨æŸ¥æ‰¾è¿æ¶¨å¤©æ•°bug"""
    print("\n" + "="*70)
    print("è¿æ¶¨å¤©æ•°ä¸“é¡¹æ£€æŸ¥")
    print("="*70)
    
    stock_dir = get_stock_data_dir()
    stock_files = list(stock_dir.glob("*.csv"))[:max_files]
    
    bug_files = []
    
    for file_path in stock_files:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        if 'è¿æ¶¨å¤©æ•°' not in df.columns:
            continue
        
        pct_change = df['æ¶¨è·Œå¹…']
        consecutive = df['è¿æ¶¨å¤©æ•°']
        
        bugs = []
        
        for idx in range(len(df)):
            if idx >= len(pct_change) or idx >= len(consecutive):
                continue
                
            pct = pct_change.iloc[idx]
            consec = consecutive.iloc[idx]
            
            if pd.isna(pct) or pd.isna(consec):
                continue
            
            # æ£€æŸ¥é€»è¾‘é”™è¯¯
            if pct > 0 and consec <= 0:
                bugs.append(f'idx={idx}: æ¶¨{pct:.2f}% ä½†è¿æ¶¨å¤©æ•°={consec}')
            elif pct < 0 and consec != 0:
                bugs.append(f'idx={idx}: è·Œ{pct:.2f}% ä½†è¿æ¶¨å¤©æ•°={consec}')
        
        if bugs:
            bug_files.append({
                'file': file_path.name,
                'bugs': bugs[:10]
            })
    
    if bug_files:
        print(f"\nå‘ç° {len(bug_files)} ä¸ªæ–‡ä»¶å­˜åœ¨è¿æ¶¨å¤©æ•°bug:")
        for bf in bug_files[:10]:
            print(f"\nğŸ“ {bf['file']}")
            for bug in bf['bugs'][:5]:
                print(f"   {bug}")
        
        if len(bug_files) > 10:
            print(f"\n... è¿˜æœ‰ {len(bug_files) - 10} ä¸ªæ–‡ä»¶å­˜åœ¨é—®é¢˜")
    else:
        print(f"\nâœ… æ²¡æœ‰å‘ç°è¿æ¶¨å¤©æ•°bug")
    
    return bug_files


def check_vol_ma_indicators(max_files: int = 30) -> list:
    """æ£€æŸ¥æˆäº¤é‡MAæŒ‡æ ‡"""
    print("\n" + "="*70)
    print("æˆäº¤é‡MAæŒ‡æ ‡æ£€æŸ¥")
    print("="*70)
    
    stock_dir = get_stock_data_dir()
    stock_files = list(stock_dir.glob("*.csv"))[:max_files]
    
    error_files = []
    
    for file_path in stock_files:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        if 'æˆäº¤é‡' not in df.columns:
            continue
        
        volume = df['æˆäº¤é‡']
        
        for window in [5, 10, 20]:
            col_name = f'VOL_MA{window}'
            if col_name not in df.columns:
                if len(df) >= window:
                    error_files.append({
                        'file': file_path.name,
                        'issue': f'ç¼ºå°‘ {col_name} åˆ—'
                    })
                continue
            
            actual = df[col_name]
            expected = volume.rolling(window=window).mean().round(2)
            diff = (actual.fillna(0) - expected.fillna(0)).abs().sum()
            
            if diff > 0.01:
                error_files.append({
                    'file': file_path.name,
                    'issue': f'{col_name}å·®å¼‚={diff:.2f}'
                })
    
    if error_files:
        print(f"\nå‘ç° {len(error_files)} ä¸ªæ–‡ä»¶å­˜åœ¨æˆäº¤é‡MAé—®é¢˜:")
        for ef in error_files[:10]:
            print(f"   {ef['file']}: {ef['issue']}")
    else:
        print(f"\nâœ… æˆäº¤é‡MAæŒ‡æ ‡æ£€æŸ¥é€šè¿‡")
    
    return error_files


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='çœŸå®æ•°æ®æ–‡ä»¶æŠ€æœ¯æŒ‡æ ‡éªŒè¯')
    parser.add_argument('--files', type=int, default=100, help='éªŒè¯æ–‡ä»¶æ•°é‡')
    args = parser.parse_args()
    
    print("\nå¼€å§‹çœŸå®æ•°æ®æ–‡ä»¶éªŒè¯...\n")
    
    result1 = verify_all_files(args.files)
    bug_files = find_consecutive_rise_bugs(args.files)
    vol_errors = check_vol_ma_indicators(args.files)
    
    print("\n" + "="*70)
    print("æœ€ç»ˆç»“è®º")
    print("="*70)
    
    if result1['all_passed'] and not bug_files and not vol_errors:
        print(f"ğŸ‰ å…¨éƒ¨éªŒè¯é€šè¿‡ï¼å…±éªŒè¯ {args.files} ä¸ªæ–‡ä»¶ï¼Œæ•°æ®å®Œå…¨æ­£ç¡®ã€‚")
        exit(0)
    else:
        print("âš ï¸ å­˜åœ¨é—®é¢˜éœ€è¦ä¿®å¤:")
        if not result1['all_passed']:
            print(f"  - æ€»ä½“éªŒè¯å¤±è´¥: {result1['total_errors']} ä¸ªé”™è¯¯")
        if bug_files:
            print(f"  - è¿æ¶¨å¤©æ•°bug: {len(bug_files)} ä¸ªæ–‡ä»¶")
        if vol_errors:
            print(f"  - æˆäº¤é‡MAé—®é¢˜: {len(vol_errors)} ä¸ªæ–‡ä»¶")
        
        print("\néœ€è¦ä¿®å¤ load_base_data.py ä¸­çš„é€»è¾‘...")
        exit(1)
